{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "# import polars as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "\n",
    "    PREPROCESS = False\n",
    "    EPOCHS = 30 #20\n",
    "    BATCH_SIZE = 4096\n",
    "    LR = 1e-3\n",
    "    WD = 1e-6\n",
    "\n",
    "    NBR_FOLDS = 2\n",
    "    SELECTED_FOLDS = [0]\n",
    "\n",
    "    SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "import torch\n",
    "def set_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    #tf.random.set_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seeds(seed=CFG.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.PREPROCESS:\n",
    "    enc = {'l': 1, 'y': 2, '@': 3, '3': 4, 'H': 5, 'S': 6, 'F': 7, 'C': 8, 'r': 9, 's': 10, '/': 11, 'c': 12, 'o': 13,\n",
    "           '+': 14, 'I': 15, '5': 16, '(': 17, '2': 18, ')': 19, '9': 20, 'i': 21, '#': 22, '6': 23, '8': 24, '4': 25, '=': 26,\n",
    "           '1': 27, 'O': 28, '[': 29, 'D': 30, 'B': 31, ']': 32, 'N': 33, '7': 34, 'n': 35, '-': 36}\n",
    "    train_raw = pd.read_parquet('data/train.parquet')\n",
    "    smiles = train_raw[train_raw['protein_name']=='BRD4']['molecule_smiles'].values\n",
    "    assert (smiles!=train_raw[train_raw['protein_name']=='HSA']['molecule_smiles'].values).sum() == 0\n",
    "    assert (smiles!=train_raw[train_raw['protein_name']=='sEH']['molecule_smiles'].values).sum() == 0\n",
    "    def encode_smile(smile):\n",
    "        tmp = [enc[i] for i in smile]\n",
    "        tmp = tmp + [0]*(142-len(tmp))\n",
    "        return np.array(tmp).astype(np.uint8)\n",
    "\n",
    "    smiles_enc = joblib.Parallel(n_jobs=60)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n",
    "    smiles_enc = np.stack(smiles_enc)\n",
    "    train = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n",
    "    train['bind1'] = train_raw[train_raw['protein_name']=='BRD4']['binds'].values\n",
    "    train['bind2'] = train_raw[train_raw['protein_name']=='HSA']['binds'].values\n",
    "    train['bind3'] = train_raw[train_raw['protein_name']=='sEH']['binds'].values\n",
    "    train.to_parquet('output/train_enc.parquet')\n",
    "\n",
    "    test_raw = pd.read_parquet('data/test.parquet')\n",
    "    smiles = test_raw['molecule_smiles'].values\n",
    "\n",
    "    smiles_enc = joblib.Parallel(n_jobs=60)(joblib.delayed(encode_smile)(smile) for smile in tqdm(smiles))\n",
    "    smiles_enc = np.stack(smiles_enc)\n",
    "    test = pd.DataFrame(smiles_enc, columns = [f'enc{i}' for i in range(142)])\n",
    "    test.to_parquet('output/test_enc.parquet')\n",
    "\n",
    "else:\n",
    "    train = pd.read_parquet('output/train_enc.parquet')\n",
    "    test = pd.read_parquet('output/test_enc.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(self, input_dim=142, input_dim_embedding=37, hidden_dim=128, num_filters=32, output_dim=3, lr=1e-3, weight_decay=1e-6):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.hparams.input_dim_embedding, embedding_dim=self.hparams.hidden_dim, padding_idx=0)\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.hparams.hidden_dim, out_channels=self.hparams.num_filters, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.hparams.num_filters, out_channels=self.hparams.num_filters*2, kernel_size=3, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv1d(in_channels=self.hparams.num_filters*2, out_channels=self.hparams.num_filters*3, kernel_size=3, stride=1, padding=0)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc1 = nn.Linear(self.hparams.num_filters*3, 1024)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "        self.output = nn.Linear(512, self.hparams.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).permute(0,2,1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.global_max_pool(x).squeeze(2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用KFold方法\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# FEATURES = [f'enc{i}' for i in range(142)]\n",
    "# TARGETS = ['bind1', 'bind2', 'bind3']\n",
    "# skf = StratifiedKFold(n_splits=CFG.NBR_FOLDS, shuffle=True, random_state=42)\n",
    "# all_preds = []\n",
    "# # Step 2: Convert the DataFrame to a NumPy array and then to a Tensor\n",
    "# test_tensor = torch.tensor(test.loc[:100,FEATURES].values, dtype=torch.int) # Use the appropriate dtype for your model\n",
    "\n",
    "\n",
    "# for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train[TARGETS].sum(1))):\n",
    "#     if fold not in CFG.SELECTED_FOLDS:\n",
    "#         continue\n",
    "\n",
    "#     # Convert pandas dataframes to PyTorch tensors\n",
    "#     X_train = torch.tensor(train.loc[train_idx, FEATURES].values, dtype=torch.int)\n",
    "#     y_train = torch.tensor(train.loc[train_idx, TARGETS].values, dtype=torch.float16)\n",
    "#     X_val = torch.tensor(train.loc[valid_idx, FEATURES].values, dtype=torch.int)\n",
    "#     y_val = torch.tensor(train.loc[valid_idx, TARGETS].values, dtype=torch.float16)\n",
    "\n",
    "    \n",
    "#     # Create TensorDatasets\n",
    "#     train_dataset = TensorDataset(X_train, y_train)\n",
    "#     valid_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "#     # Create DataLoaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True)\n",
    "#     valid_loader = DataLoader(valid_dataset, batch_size=CFG.BATCH_SIZE)\n",
    "        \n",
    "#     model = MyModel(lr=CFG.LR, weight_decay=CFG.WD)\n",
    "\n",
    "#     early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=True)\n",
    "#     checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", dirpath=\"./ckpoint\", filename=f\"model-{fold}\", save_top_k=1, mode=\"min\")\n",
    "#     lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=CFG.EPOCHS,\n",
    "#         callbacks=[early_stop_callback, checkpoint_callback, lr_monitor],\n",
    "#         devices=1,\n",
    "#         accelerator=\"gpu\",  # Adjust based on your hardware\n",
    "#         enable_progress_bar=True,\n",
    "#     )\n",
    "\n",
    "#     # trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "\n",
    "#     model = MyModel.load_from_checkpoint(\"D:/zyh/bddm/protein_predict/ckpoint/model-0-v4.ckpt\").to(\"cpu\")\n",
    "#     # oof = model(X_val)\n",
    "#     # print('fold :', fold, 'CV score =', average_precision_score(y_val, oof, average='micro'))\n",
    "\n",
    "#     preds = model(test_tensor)\n",
    "#     all_preds.append(preds)\n",
    "\n",
    "# preds = torch.mean(torch.stack(all_preds), 0).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load ok\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Progress: 100%|██████████| 4806/4806 [02:23<00:00, 33.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19683122, 3])\n",
      "torch.Size([19683122, 3])\n",
      "Score = 0.6586045653962077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 使用正常训练\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "FEATURES = [f'enc{i}' for i in range(142)]\n",
    "TARGETS = ['bind1', 'bind2', 'bind3']\n",
    "all_preds = []\n",
    "\n",
    "\n",
    "# Train-validation split\n",
    "train_idx, valid_idx = train_test_split(train.index, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert pandas dataframes to PyTorch tensors\n",
    "X_train = torch.tensor(train.loc[train_idx, FEATURES].values, dtype=torch.int)\n",
    "y_train = torch.tensor(train.loc[train_idx, TARGETS].values, dtype=torch.float16)\n",
    "X_val = torch.tensor(train.loc[valid_idx, FEATURES].values, dtype=torch.int)\n",
    "y_val = torch.tensor(train.loc[valid_idx, TARGETS].values, dtype=torch.float16)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG.BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=CFG.BATCH_SIZE)\n",
    "\n",
    "# Initialize the model\n",
    "model = MyModel(lr=CFG.LR, weight_decay=CFG.WD)\n",
    "\n",
    "# Define callbacks\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=True)\n",
    "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", dirpath=\"./ckpoint/2\", filename=\"model\", save_top_k=1, mode=\"min\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "# Trainer setup\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=CFG.EPOCHS,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback, lr_monitor],\n",
    "    devices=1,\n",
    "    accelerator=\"gpu\",  # Adjust based on your hardware\n",
    "    enable_progress_bar=True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)\n",
    "\n",
    "\n",
    "# Load model onto the GPU\n",
    "# model = MyModel.load_from_checkpoint(\"D:/zyh/bddm/protein_predict/ckpoint/model.ckpt\").to(device)\n",
    "model = MyModel.load_from_checkpoint(\"./ckpoint/CNN_1/model.ckpt\").to(device)\n",
    "print(\"load ok\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Perform batched inference with data also on GPU\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for batch in tqdm(valid_loader, desc=\"Validation Progress\"):\n",
    "        # Move data to GPU\n",
    "        batch_X, batch_y = batch[0].to(device), batch[1].to(device)\n",
    "        \n",
    "        # Predict for the batch\n",
    "        preds = model(batch_X)\n",
    "        \n",
    "        # Move predictions and targets to CPU for concatenation\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_targets.append(batch_y.cpu())\n",
    "\n",
    "# Concatenate all batches into single tensors\n",
    "all_preds = torch.cat(all_preds, dim=0)\n",
    "print(all_preds.shape)\n",
    "all_targets = torch.cat(all_targets, dim=0)\n",
    "print(all_targets.shape)\n",
    "\n",
    "# Compute the score\n",
    "score = average_precision_score(all_targets.numpy(), all_preds.numpy(), average='micro')\n",
    "print('Score =', score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -8.6193,  -7.9725, -11.6067])\n",
      "tensor([0., 0., 0.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(all_preds[3])\n",
    "print(all_targets[3])\n",
    "\n",
    "# # 要查找的目标 tensors\n",
    "# target_1 = torch.tensor([0., 0., 1.], dtype=torch.float16)\n",
    "# target_2 = torch.tensor([0., 1., 0.], dtype=torch.float16)\n",
    "\n",
    "# # 检查是否有这两个 tensor\n",
    "# found_target_1 = any(torch.equal(tensor, target_1) for tensor in all_targets)\n",
    "# found_target_2 = any(torch.equal(tensor, target_2) for tensor in all_targets)\n",
    "\n",
    "# if found_target_1 or found_target_2:\n",
    "#     print(\"Found tensor([0., 0., 1.]) or tensor([0., 1., 0.]) in all_targets.\")\n",
    "# else:\n",
    "#     print(\"Neither tensor([0., 0., 1.]) nor tensor([0., 1., 0.]) was found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Data Inference Progress: 100%|██████████| 409/409 [00:08<00:00, 49.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Predictions shape: (1674896, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Convert the DataFrame to a NumPy array and then to a Tensor\n",
    "test_tensor = torch.tensor(test.loc[:,FEATURES].values, dtype=torch.int) # Use the appropriate dtype for your model\n",
    "\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "test_loader = DataLoader(test_dataset,batch_size=CFG.BATCH_SIZE,shuffle=False)\n",
    "# preds = model(test_tensor)\n",
    "# all_preds.append(preds)\n",
    "\n",
    "# Perform batched inference\n",
    "all_preds_test = []\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for batch in tqdm(test_loader, desc=\"Test Data Inference Progress\"):\n",
    "        # Move batch to GPU\n",
    "        batch_X = batch[0].to(device)\n",
    "        \n",
    "        # Predict for the batch\n",
    "        preds = model(batch_X)\n",
    "        \n",
    "        # Move predictions to CPU and append to results\n",
    "        all_preds_test.append(preds.cpu())  \n",
    "\n",
    "# Concatenate all predictions into a single tensor\n",
    "final_preds = torch.cat(all_preds_test, dim=0)\n",
    "\n",
    "# Min-Max Normalization to scale predictions between 0 and 1\n",
    "min_val = final_preds.min()\n",
    "max_val = final_preds.max()\n",
    "final_preds = (final_preds - min_val) / (max_val - min_val)\n",
    "\n",
    "# Convert to NumPy for further processing\n",
    "final_preds = final_preds.numpy()\n",
    "\n",
    "# Output the final predictions\n",
    "print(\"Inference completed. Predictions shape:\", final_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38889262 0.44838738 0.32235193]\n"
     ]
    }
   ],
   "source": [
    "print(final_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_56004\\1989673664.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.38889262 0.35434645 0.3408568  ... 0.28869963 0.40996334 0.3572941 ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  tst.loc[tst['protein_name']=='BRD4', 'binds'] = final_preds[(tst['protein_name']=='BRD4').values, 0]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "tst = pd.read_parquet('data/test.parquet')\n",
    "tst['binds'] = 0\n",
    "tst.loc[tst['protein_name']=='BRD4', 'binds'] = final_preds[(tst['protein_name']=='BRD4').values, 0]\n",
    "tst.loc[tst['protein_name']=='HSA', 'binds'] = final_preds[(tst['protein_name']=='HSA').values, 1]\n",
    "tst.loc[tst['protein_name']=='sEH', 'binds'] = final_preds[(tst['protein_name']=='sEH').values, 2]\n",
    "tst[['id', 'binds']].to_csv('submission_CNN.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
